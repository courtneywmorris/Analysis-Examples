{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pickle as pkl\n",
    "from pyspark.mllib.feature import HashingTF, IDF, Word2Vec\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_raw = sc.textFile('s3n://[YOUR_AWS_ACCESS_KEY_ID]:[YOUR_AWS_SECRET_ACCESS_KEY]@sparkdatasets/news.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw.getNumPartitions()\n",
    "data_raw = data_raw.repartition(4)\n",
    "data_raw.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_tuple(x, keys):\n",
    "    x_dict = json.loads(x)\n",
    "    return tuple(x_dict[k] for k in keys)\n",
    "\n",
    "label_name_rdd = data_raw\\\n",
    "    .map(lambda x: extract_tuple(x, ['label', 'label_name']))\n",
    "    \n",
    "text_rdd = data_raw\\\n",
    "    .map(lambda x: extract_tuple(x, ['label', 'text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopset = stopwords.words('english')\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "preprocessed_text_rdd = text_rdd\\\n",
    "    .mapValues(lambda x: x.lower().encode('ascii', 'ignore'))\\\n",
    "    .mapValues(lambda x: regex.sub(' ', x))\\\n",
    "    .mapValues(lambda x: [word for word in nltk.word_tokenize(x) \n",
    "                    if word not in stopset])\\\n",
    "    .mapValues(lambda x: [stemmer.stem(w) for w in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = preprocessed_text_rdd.flatMap(lambda x: x[1]).distinct().collect()\n",
    "reverse_vocab = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "def get_tf(word_list):\n",
    "    word_count = Counter(word_list)\n",
    "    tf_vector = [0]*len(vocab)\n",
    "    for word in word_count:\n",
    "        tf_vector[reverse_vocab[word]] = word_count[word]\n",
    "    return tf_vector\n",
    "\n",
    "tf_rdd = preprocessed_text_rdd.mapValues(get_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "htf = HashingTF(10000)\n",
    "word_vecs_rdd = preprocessed_text_rdd.mapValues(htf.transform).cache()\n",
    "\n",
    "labeled_point_rdd = word_vecs_rdd.map(lambda (target, feature): LabeledPoint(target, feature))\n",
    "labeled_point_rdd.setName('labeled_point').persist()\n",
    "\n",
    "train_rdd, test_rdd = labeled_point_rdd.randomSplit([70, 30])\n",
    "\n",
    "model = NaiveBayes.train(train_rdd)\n",
    "\n",
    "y = np.array(test_rdd.map(lambda x: x.label).collect())\n",
    "y_pred = np.array(test_rdd.map(lambda x: model.predict(x.features)).collect())\n",
    "print (y == y_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.textFile('s3n://[YOUR_AWS_ACCESS_KEY_ID]:[YOUR_AWS_SECRET_ACCESS_KEY]@sparkdatasets/text8_lines')\n",
    "word2vec = Word2Vec()\n",
    "word2vec = word2vec.fit(preprocessed_text_rdd.map(lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vocab[100:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.0052, 0.0531, -0.0075, 0.004, -0.0089, 0.0484, 0.0201, 0.0092, 0.0032, -0.0249, -0.0277, 0.0174, -0.0896, -0.0166, 0.0233, -0.02, -0.0321, -0.0299, 0.042, 0.1446, 0.0206, 0.0009, 0.0218, -0.0082, -0.0165, -0.0187, -0.0502, -0.0431, 0.0476, 0.1121, -0.0419, 0.0265, -0.0564, 0.0503, 0.0781, -0.0062, -0.0451, 0.0308, 0.006, -0.0292, 0.0678, -0.0421, 0.0584, -0.035, 0.022, -0.024, 0.0134, -0.0717, -0.0062, -0.0593, 0.0607, -0.006, -0.0495, 0.0107, 0.0209, -0.0143, -0.0877, 0.0508, 0.0031, -0.0167, -0.0248, -0.0265, 0.013, -0.0132, -0.0197, -0.0167, 0.0526, 0.027, 0.0763, -0.046, 0.0436, 0.0112, -0.1059, 0.0299, 0.0222, -0.0504, 0.0137, -0.053, 0.1092, -0.0377, -0.0254, 0.0012, -0.1183, -0.0051, 0.0975, 0.0424, -0.0492, -0.0619, 0.0404, 0.0155, -0.0051, 0.0415, -0.0327, -0.0141, 0.0319, -0.0802, -0.1111, -0.0101, -0.0165, -0.0136])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.transform('neurologist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
